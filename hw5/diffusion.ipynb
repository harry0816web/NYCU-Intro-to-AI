{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Model Training for Anime Face Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "Run the cell below to install the required dependencies. You can skip this step if the environment is already setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T07:59:13.920942Z",
     "iopub.status.busy": "2025-06-12T07:59:13.920652Z",
     "iopub.status.idle": "2025-06-12T07:59:31.776940Z",
     "shell.execute_reply": "2025-06-12T07:59:31.775951Z",
     "shell.execute_reply.started": "2025-06-12T07:59:13.920921Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install einops\n",
    "!pip install transformers\n",
    "!pip install ema_pytorch\n",
    "!pip install accelerate\n",
    "!pip install pytorch_fid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies and Misc Setup\n",
    "We import packages we need and define some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T07:59:31.778561Z",
     "iopub.status.busy": "2025-06-12T07:59:31.778228Z",
     "iopub.status.idle": "2025-06-12T07:59:38.887180Z",
     "shell.execute_reply": "2025-06-12T07:59:38.886541Z",
     "shell.execute_reply.started": "2025-06-12T07:59:31.778539Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Module, ModuleList\n",
    "from torch.amp import autocast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as T, utils\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from ema_pytorch import EMA\n",
    "\n",
    "from accelerate import Accelerator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T07:59:38.888983Z",
     "iopub.status.busy": "2025-06-12T07:59:38.888664Z",
     "iopub.status.idle": "2025-06-12T07:59:38.892532Z",
     "shell.execute_reply": "2025-06-12T07:59:38.891622Z",
     "shell.execute_reply.started": "2025-06-12T07:59:38.888962Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T07:59:38.894161Z",
     "iopub.status.busy": "2025-06-12T07:59:38.893863Z",
     "iopub.status.idle": "2025-06-12T07:59:38.912838Z",
     "shell.execute_reply": "2025-06-12T07:59:38.912034Z",
     "shell.execute_reply.started": "2025-06-12T07:59:38.894132Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if callable(d) else d\n",
    "\n",
    "def cast_tuple(t, length = 1):\n",
    "    if isinstance(t, tuple):\n",
    "        return t\n",
    "    return ((t,) * length)\n",
    "\n",
    "def divisible_by(numer, denom):\n",
    "    return (numer % denom) == 0\n",
    "\n",
    "def identity(t, *args, **kwargs):\n",
    "    return t\n",
    "\n",
    "def cycle(dl):\n",
    "    while True:\n",
    "        for data in dl:\n",
    "            yield data\n",
    "\n",
    "def normalize_to_neg_one_to_one(img):\n",
    "    return img * 2 - 1\n",
    "\n",
    "def unnormalize_to_zero_to_one(t):\n",
    "    return (t + 1) * 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T07:59:38.914131Z",
     "iopub.status.busy": "2025-06-12T07:59:38.913844Z",
     "iopub.status.idle": "2025-06-12T07:59:39.057121Z",
     "shell.execute_reply": "2025-06-12T07:59:39.056344Z",
     "shell.execute_reply.started": "2025-06-12T07:59:38.914101Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def Upsample(dim, dim_out = None):\n",
    "    return nn.Sequential(\n",
    "        nn.Upsample(scale_factor = 2, mode = 'nearest'),\n",
    "        nn.Conv2d(dim, default(dim_out, dim), 3, padding = 1)\n",
    "    )\n",
    "\n",
    "def Downsample(dim, dim_out = None):\n",
    "    return nn.Sequential(\n",
    "        Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1 = 2, p2 = 2),\n",
    "        nn.Conv2d(dim * 4, default(dim_out, dim), 1)\n",
    "    )\n",
    "\n",
    "class RMSNorm(Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.scale = dim ** 0.5\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.normalize(x, dim = 1) * self.g * self.scale\n",
    "\n",
    "# sinusoidal positional embeds\n",
    "\n",
    "class SinusoidalPosEmb(Module):\n",
    "    def __init__(self, dim, theta = 10000):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.theta = theta\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(self.theta) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
    "        emb = x[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "        return emb\n",
    "\n",
    "class RandomOrLearnedSinusoidalPosEmb(Module):\n",
    "    \"\"\" following @crowsonkb 's lead with random (learned optional) sinusoidal pos emb \"\"\"\n",
    "    \"\"\" https://github.com/crowsonkb/v-diffusion-jax/blob/master/diffusion/models/danbooru_128.py#L8 \"\"\"\n",
    "\n",
    "    def __init__(self, dim, is_random = False):\n",
    "        super().__init__()\n",
    "        assert divisible_by(dim, 2)\n",
    "        half_dim = dim // 2\n",
    "        self.weights = nn.Parameter(torch.randn(half_dim), requires_grad = not is_random)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = rearrange(x, 'b -> b 1')\n",
    "        freqs = x * rearrange(self.weights, 'd -> 1 d') * 2 * math.pi\n",
    "        fouriered = torch.cat((freqs.sin(), freqs.cos()), dim = -1)\n",
    "        fouriered = torch.cat((x, fouriered), dim = -1)\n",
    "        return fouriered\n",
    "\n",
    "# building block modules\n",
    "\n",
    "class Block(Module):\n",
    "    def __init__(self, dim, dim_out, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(dim, dim_out, 3, padding = 1)\n",
    "        self.norm = RMSNorm(dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, scale_shift = None):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if exists(scale_shift):\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "\n",
    "        x = self.act(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class ResnetBlock(Module):\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim = None, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, dim_out * 2)\n",
    "        ) if exists(time_emb_dim) else None\n",
    "\n",
    "        self.block1 = Block(dim, dim_out, dropout = dropout)\n",
    "        self.block2 = Block(dim_out, dim_out)\n",
    "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x, time_emb = None):\n",
    "\n",
    "        scale_shift = None\n",
    "        if exists(self.mlp) and exists(time_emb):\n",
    "            time_emb = self.mlp(time_emb)\n",
    "            time_emb = rearrange(time_emb, 'b c -> b c 1 1')\n",
    "            scale_shift = time_emb.chunk(2, dim = 1)\n",
    "\n",
    "        h = self.block1(x, scale_shift = scale_shift)\n",
    "\n",
    "        h = self.block2(h)\n",
    "\n",
    "        return h + self.res_conv(x)\n",
    "\n",
    "class Attend(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dropout = 0.,\n",
    "        flash = False,\n",
    "        scale = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.scale = scale\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.flash = flash\n",
    "\n",
    "        AttentionConfig = namedtuple('AttentionConfig', ['enable_flash', 'enable_math', 'enable_mem_efficient'])\n",
    "        \n",
    "        self.cpu_config = AttentionConfig(True, True, True)\n",
    "        self.cuda_config = None\n",
    "\n",
    "        if not torch.cuda.is_available() or not flash:\n",
    "            return\n",
    "\n",
    "        self.cuda_config = AttentionConfig(False, True, True)\n",
    "\n",
    "    def flash_attn(self, q, k, v):\n",
    "        _, heads, q_len, _, k_len, is_cuda, device = *q.shape, k.shape[-2], q.is_cuda, q.device\n",
    "\n",
    "        if exists(self.scale):\n",
    "            default_scale = q.shape[-1]\n",
    "            q = q * (self.scale / default_scale)\n",
    "\n",
    "        q, k, v = map(lambda t: t.contiguous(), (q, k, v))\n",
    "\n",
    "        # Check if there is a compatible device for flash attention\n",
    "\n",
    "        config = self.cuda_config if is_cuda else self.cpu_config\n",
    "\n",
    "        # pytorch 2.0 flash attn: q, k, v, mask, dropout, causal, softmax_scale\n",
    "\n",
    "        with torch.backends.cuda.sdp_kernel(**config._asdict()):\n",
    "            out = F.scaled_dot_product_attention(\n",
    "                q, k, v,\n",
    "                dropout_p = self.dropout if self.training else 0.\n",
    "            )\n",
    "\n",
    "        return out\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        \"\"\"\n",
    "        einstein notation\n",
    "        b - batch\n",
    "        h - heads\n",
    "        n, i, j - sequence length (base sequence length, source, target)\n",
    "        d - feature dimension\n",
    "        \"\"\"\n",
    "\n",
    "        q_len, k_len, device = q.shape[-2], k.shape[-2], q.device\n",
    "\n",
    "        if self.flash:\n",
    "            return self.flash_attn(q, k, v)\n",
    "\n",
    "        scale = default(self.scale, q.shape[-1] ** -0.5)\n",
    "\n",
    "        # similarity\n",
    "\n",
    "        sim = einsum(f\"b h i d, b h j d -> b h i j\", q, k) * scale\n",
    "\n",
    "        # attention\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "\n",
    "        # aggregate values\n",
    "\n",
    "        out = einsum(f\"b h i j, b h j d -> b h i d\", attn, v)\n",
    "\n",
    "        return out\n",
    "\n",
    "class LinearAttention(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        heads = 4,\n",
    "        dim_head = 32,\n",
    "        num_mem_kv = 4\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "\n",
    "        self.norm = RMSNorm(dim)\n",
    "\n",
    "        self.mem_kv = nn.Parameter(torch.randn(2, heads, dim_head, num_mem_kv))\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, dim, 1),\n",
    "            RMSNorm(dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = 1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h = self.heads), qkv)\n",
    "\n",
    "        mk, mv = map(lambda t: repeat(t, 'h c n -> b h c n', b = b), self.mem_kv)\n",
    "        k, v = map(partial(torch.cat, dim = -1), ((mk, k), (mv, v)))\n",
    "\n",
    "        q = q.softmax(dim = -2)\n",
    "        k = k.softmax(dim = -1)\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        context = torch.einsum('b h d n, b h e n -> b h d e', k, v)\n",
    "\n",
    "        out = torch.einsum('b h d e, b h d n -> b h e n', context, q)\n",
    "        out = rearrange(out, 'b h c (x y) -> b (h c) x y', h = self.heads, x = h, y = w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Attention(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        heads = 4,\n",
    "        dim_head = 32,\n",
    "        num_mem_kv = 4,\n",
    "        flash = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "\n",
    "        self.norm = RMSNorm(dim)\n",
    "        self.attend = Attend(flash = flash)\n",
    "\n",
    "        self.mem_kv = nn.Parameter(torch.randn(2, heads, num_mem_kv, dim_head))\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = 1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> b h (x y) c', h = self.heads), qkv)\n",
    "\n",
    "        mk, mv = map(lambda t: repeat(t, 'h n d -> b h n d', b = b), self.mem_kv)\n",
    "        k, v = map(partial(torch.cat, dim = -2), ((mk, k), (mv, v)))\n",
    "\n",
    "        out = self.attend(q, k, v)\n",
    "\n",
    "        out = rearrange(out, 'b h (x y) d -> b (h d) x y', x = h, y = w)\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T07:59:39.058296Z",
     "iopub.status.busy": "2025-06-12T07:59:39.058055Z",
     "iopub.status.idle": "2025-06-12T07:59:39.081485Z",
     "shell.execute_reply": "2025-06-12T07:59:39.080702Z",
     "shell.execute_reply.started": "2025-06-12T07:59:39.058276Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "########## Unet Architecture ##########\n",
    "class Unet(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        init_dim = None,\n",
    "        out_dim = None,\n",
    "        dim_mults = (1, 2, 4),\n",
    "        channels = 3,\n",
    "        self_condition = False,\n",
    "        learned_variance = False,\n",
    "        learned_sinusoidal_cond = False,\n",
    "        random_fourier_features = False,\n",
    "        learned_sinusoidal_dim = 16,\n",
    "        sinusoidal_pos_emb_theta = 10000,\n",
    "        dropout = 0.,\n",
    "        attn_dim_head = 32,\n",
    "        attn_heads = 4,\n",
    "        full_attn = None,    # defaults to full attention only for inner most layer\n",
    "        flash_attn = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # determine dimensions\n",
    "\n",
    "        self.channels = channels\n",
    "        self.self_condition = self_condition\n",
    "        input_channels = channels * (2 if self_condition else 1)\n",
    "\n",
    "        init_dim = default(init_dim, dim)\n",
    "        self.init_conv = nn.Conv2d(input_channels, init_dim, 7, padding = 3)\n",
    "\n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "\n",
    "        # time embeddings\n",
    "\n",
    "        time_dim = dim * 4\n",
    "\n",
    "        self.random_or_learned_sinusoidal_cond = learned_sinusoidal_cond or random_fourier_features\n",
    "\n",
    "        if self.random_or_learned_sinusoidal_cond:\n",
    "            sinu_pos_emb = RandomOrLearnedSinusoidalPosEmb(learned_sinusoidal_dim, random_fourier_features)\n",
    "            fourier_dim = learned_sinusoidal_dim + 1\n",
    "        else:\n",
    "            sinu_pos_emb = SinusoidalPosEmb(dim, theta = sinusoidal_pos_emb_theta)\n",
    "            fourier_dim = dim\n",
    "\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            sinu_pos_emb,\n",
    "            nn.Linear(fourier_dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim)\n",
    "        )\n",
    "\n",
    "        # attention\n",
    "\n",
    "        if not full_attn:\n",
    "            full_attn = (*((False,) * (len(dim_mults) - 1)), True)\n",
    "\n",
    "        num_stages = len(dim_mults)\n",
    "        full_attn  = cast_tuple(full_attn, num_stages)\n",
    "        attn_heads = cast_tuple(attn_heads, num_stages)\n",
    "        attn_dim_head = cast_tuple(attn_dim_head, num_stages)\n",
    "\n",
    "        assert len(full_attn) == len(dim_mults)\n",
    "\n",
    "        # prepare blocks\n",
    "\n",
    "        FullAttention = partial(Attention, flash = flash_attn)\n",
    "        resnet_block = partial(ResnetBlock, time_emb_dim = time_dim, dropout = dropout)\n",
    "\n",
    "        # layers\n",
    "\n",
    "        self.downs = ModuleList([])\n",
    "        self.ups = ModuleList([])\n",
    "        num_resolutions = len(in_out)\n",
    "\n",
    "        for ind, ((dim_in, dim_out), layer_full_attn, layer_attn_heads, layer_attn_dim_head) in enumerate(zip(in_out, full_attn, attn_heads, attn_dim_head)):\n",
    "            is_last = ind >= (num_resolutions - 1)\n",
    "\n",
    "            attn_klass = FullAttention if layer_full_attn else LinearAttention\n",
    "\n",
    "            self.downs.append(ModuleList([\n",
    "                resnet_block(dim_in, dim_in),\n",
    "                attn_klass(dim_in, dim_head = layer_attn_dim_head, heads = layer_attn_heads),\n",
    "                Downsample(dim_in, dim_out) if not is_last else nn.Conv2d(dim_in, dim_out, 3, padding = 1)\n",
    "            ]))\n",
    "\n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = resnet_block(mid_dim, mid_dim)\n",
    "        self.mid_attn = FullAttention(mid_dim, heads = attn_heads[-1], dim_head = attn_dim_head[-1])\n",
    "        self.mid_block2 = resnet_block(mid_dim, mid_dim)\n",
    "\n",
    "        for ind, ((dim_in, dim_out), layer_full_attn, layer_attn_heads, layer_attn_dim_head) in enumerate(zip(*map(reversed, (in_out, full_attn, attn_heads, attn_dim_head)))):\n",
    "            is_last = ind == (len(in_out) - 1)\n",
    "\n",
    "            attn_klass = FullAttention if layer_full_attn else LinearAttention\n",
    "\n",
    "            self.ups.append(ModuleList([\n",
    "                resnet_block(dim_out + dim_in, dim_out),\n",
    "                attn_klass(dim_out, dim_head = layer_attn_dim_head, heads = layer_attn_heads),\n",
    "                Upsample(dim_out, dim_in) if not is_last else  nn.Conv2d(dim_out, dim_in, 3, padding = 1)\n",
    "            ]))\n",
    "\n",
    "        default_out_dim = channels * (1 if not learned_variance else 2)\n",
    "        self.out_dim = default(out_dim, default_out_dim)\n",
    "\n",
    "        self.final_res_block = resnet_block(init_dim * 2, init_dim)\n",
    "        self.final_conv = nn.Conv2d(init_dim, self.out_dim, 1)\n",
    "\n",
    "    @property\n",
    "    def downsample_factor(self):\n",
    "        return 2 ** (len(self.downs) - 1)\n",
    "\n",
    "    def forward(self, x, time, x_self_cond = None):\n",
    "        assert all([divisible_by(d, self.downsample_factor) for d in x.shape[-2:]]), f'your input dimensions {x.shape[-2:]} need to be divisible by {self.downsample_factor}, given the unet'\n",
    "\n",
    "        if self.self_condition:\n",
    "            x_self_cond = default(x_self_cond, lambda: torch.zeros_like(x))\n",
    "            x = torch.cat((x_self_cond, x), dim = 1)\n",
    "\n",
    "        x = self.init_conv(x)\n",
    "        r = x.clone()\n",
    "\n",
    "        t = self.time_mlp(time)\n",
    "\n",
    "        h = []\n",
    "\n",
    "        for block, attn, downsample in self.downs:\n",
    "            x = block(x, t)\n",
    "            x = attn(x) + x\n",
    "            h.append(x)\n",
    "\n",
    "            x = downsample(x)\n",
    "\n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_attn(x) + x\n",
    "        x = self.mid_block2(x, t)\n",
    "\n",
    "        for block, attn, upsample in self.ups:\n",
    "            x = torch.cat((x, h.pop()), dim = 1)\n",
    "            x = block(x, t)\n",
    "            x = attn(x) + x\n",
    "\n",
    "            x = upsample(x)\n",
    "\n",
    "        x = torch.cat((x, r), dim = 1)\n",
    "\n",
    "        x = self.final_res_block(x, t)\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T07:59:39.082683Z",
     "iopub.status.busy": "2025-06-12T07:59:39.082402Z",
     "iopub.status.idle": "2025-06-12T07:59:39.122984Z",
     "shell.execute_reply": "2025-06-12T07:59:39.122142Z",
     "shell.execute_reply.started": "2025-06-12T07:59:39.082663Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "def expand(x):\n",
    "    x = x.to(torch.float32)\n",
    "    return x.view(-1, 1, 1, 1)\n",
    "\n",
    "########## Beta Schedulers ##########\n",
    "def linear_beta_schedule(timesteps):\n",
    "    \"\"\"\n",
    "    linear schedule, proposed in original ddpm paper\n",
    "    \"\"\"\n",
    "    scale = 1000 / timesteps\n",
    "    beta_start = scale * 0.0001\n",
    "    beta_end = scale * 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps, dtype = torch.float64)\n",
    "\n",
    "def cosine_beta_schedule(timesteps: int, s: float = 0.008):\n",
    "    \"\"\"\n",
    "    Cosine schedule from Nichol & Dhariwal 2021.\n",
    "    Args:\n",
    "        timesteps: total diffusion steps T\n",
    "        s: small offset to avoid singularities (default=0.008)\n",
    "    Returns:\n",
    "        betas: (T,) tensor\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps, dtype=torch.float64)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps + s) / (1 + s)) * math.pi / 2) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1. - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return betas.clamp(min=0.0, max=0.999)\n",
    "\n",
    "class GaussianDiffusion(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        *,\n",
    "        image_size,\n",
    "        timesteps = 100,\n",
    "        sampling_timesteps = None,\n",
    "        objective = 'pred_v',\n",
    "        beta_schedule = 'linear',\n",
    "        schedule_fn_kwargs = dict(),\n",
    "        ddim_sampling_eta = 0.,\n",
    "        auto_normalize = True,\n",
    "        min_snr_loss_weight = False, # https://arxiv.org/abs/2303.09556\n",
    "        min_snr_gamma = 5\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert not (type(self) == GaussianDiffusion and model.channels != model.out_dim)\n",
    "        assert not hasattr(model, 'random_or_learned_sinusoidal_cond') or not model.random_or_learned_sinusoidal_cond\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        self.channels = self.model.channels\n",
    "        self.self_condition = self.model.self_condition\n",
    "\n",
    "        if isinstance(image_size, int):\n",
    "            image_size = (image_size, image_size)\n",
    "        assert isinstance(image_size, (tuple, list)) and len(image_size) == 2, 'image size must be a integer or a tuple/list of two integers'\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.objective = objective\n",
    "\n",
    "        assert objective in {'pred_noise', 'pred_x0', 'pred_v'}, 'objective must be either pred_noise (predict noise) or pred_x0 (predict image start) or pred_v (predict v [v-parameterization as defined in appendix D of progressive distillation paper, used in imagen-video successfully])'\n",
    "\n",
    "        betas, alphas, alphas_bar = self.init(beta_schedule, schedule_fn_kwargs, timesteps)\n",
    "        alphas_bar_prev = F.pad(alphas_bar[:-1], (1, 0), value = 1.)\n",
    "\n",
    "        timesteps, = betas.shape\n",
    "        self.num_timesteps = int(timesteps)\n",
    "\n",
    "        # sampling related parameters\n",
    "\n",
    "        self.sampling_timesteps = default(sampling_timesteps, timesteps) # default num sampling timesteps to number of timesteps at training\n",
    "\n",
    "        assert self.sampling_timesteps <= timesteps\n",
    "        self.is_ddim_sampling = self.sampling_timesteps < timesteps\n",
    "        self.ddim_sampling_eta = ddim_sampling_eta\n",
    "\n",
    "        # helper function to register buffer from float64 to float32\n",
    "\n",
    "        register_buffer = lambda name, val: self.register_buffer(name, val.to(torch.float32))\n",
    "\n",
    "        register_buffer('betas', betas)\n",
    "        register_buffer('alphas', alphas) # add register !!!\n",
    "        register_buffer('alphas_bar', alphas_bar)\n",
    "        register_buffer('alphas_bar_prev', alphas_bar_prev)\n",
    "\n",
    "        # derive loss weight\n",
    "        # snr - signal noise ratio\n",
    "\n",
    "        snr = alphas_bar / (1 - alphas_bar)\n",
    "\n",
    "        # https://arxiv.org/abs/2303.09556\n",
    "\n",
    "        maybe_clipped_snr = snr.clone()\n",
    "        if min_snr_loss_weight:\n",
    "            maybe_clipped_snr.clamp_(max = min_snr_gamma)\n",
    "\n",
    "        if objective == 'pred_noise':\n",
    "            register_buffer('loss_weight', maybe_clipped_snr / snr)\n",
    "        elif objective == 'pred_x0':\n",
    "            register_buffer('loss_weight', maybe_clipped_snr)\n",
    "        elif objective == 'pred_v':\n",
    "            register_buffer('loss_weight', maybe_clipped_snr / (snr + 1))\n",
    "\n",
    "        # auto-normalization of data [0, 1] -> [-1, 1] - can turn off by setting it to be False\n",
    "\n",
    "        self.normalize = normalize_to_neg_one_to_one if auto_normalize else identity\n",
    "        self.unnormalize = unnormalize_to_zero_to_one if auto_normalize else identity\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.betas.device\n",
    "    \n",
    "    def init(self, beta_schedule, schedule_fn_kwargs, timesteps):\n",
    "        \"\"\"\n",
    "        Initialize the betas, alphas, and alphas_bar\n",
    "        \n",
    "        Args:\n",
    "            beta_schedule      (str) : the schedule function for beta\n",
    "            schedule_fn_kwargs (dict): kwargs for the schedule function\n",
    "            timesteps          (int) : the number of timesteps\n",
    "        \n",
    "        Returns:\n",
    "            betas      (torch.Tensor): the beta schedule\n",
    "            alphas     (torch.Tensor): the alphas for the diffusion chain\n",
    "            alphas_bar (torch.Tensor): the alphas_bar for the diffusion chain\n",
    "            \n",
    "        Shapes:\n",
    "            betas      : (timesteps)\n",
    "            alphas     : (timesteps)\n",
    "            alphas_bar : (timesteps)\n",
    "            \n",
    "            e.g. (100)\n",
    "        \"\"\"\n",
    "        \n",
    "        ##############################\n",
    "        # TODO1-1-1: Denoising Process - Initialization\n",
    "        # Begin your code\n",
    "        \n",
    "        if beta_schedule == 'linear':\n",
    "            beta_schedule_fn = linear_beta_schedule\n",
    "            betas = beta_schedule_fn(timesteps, **schedule_fn_kwargs)\n",
    "        else:\n",
    "            betas = cosine_beta_schedule(timesteps, **schedule_fn_kwargs)\n",
    "        \n",
    "        # alphas = 1 - betas (forward process variance retention)\n",
    "        alphas = 1.0 - betas\n",
    "        \n",
    "        # alphas_bar = cumulative product of alphas \n",
    "        alphas_bar = torch.cumprod(alphas, dim=0)\n",
    "        \n",
    "        return betas, alphas, alphas_bar\n",
    "\n",
    "        # End your code\n",
    "        ##############################\n",
    "            \n",
    "    \n",
    "    def add_noise_forward(self, x_start, t, noise):\n",
    "        \"\"\"\n",
    "        Implement the forward function\n",
    "\n",
    "        Args:\n",
    "            x_start (torch.Tensor): initial image\n",
    "            t       (torch.Tensor): timestep\n",
    "            noise   (torch.Tensor): a random noise\n",
    "        \n",
    "        Returns:\n",
    "            x_t     (torch.Tensor): noisy image at timestep t\n",
    "            \n",
    "        Shapes:\n",
    "            x_start: (batch_size, channels, IMG_SIZE, IMG_SIZE)\n",
    "            t      : (batch_size)\n",
    "            noise  : (batch_size, channels, IMG_SIZE, IMG_SIZE)\n",
    "            x_t    : (batch_size, channels, IMG_SIZE, IMG_SIZE)\n",
    "            \n",
    "            e.g. (32, 3, 64, 64)\n",
    "        \"\"\"\n",
    "        \n",
    "        ##############################\n",
    "        # TODO1-1-2: Denoising Process - Forward function\n",
    "        # Begin your code\n",
    "\n",
    "        # extract alphas_bar\n",
    "        alphas_bar_t = extract(self.alphas_bar, t, x_start.shape)\n",
    "        \n",
    "        # forward diffusion formula\n",
    "        x_t = torch.sqrt(alphas_bar_t) * x_start + torch.sqrt(1.0 - alphas_bar_t) * noise\n",
    "        \n",
    "        return x_t\n",
    "\n",
    "        # End your code\n",
    "        ##############################\n",
    "        \n",
    "    def denoise_backward(self, pred_x_start, t, t_prev, pred_noise, noise, eta):\n",
    "        \"\"\"\n",
    "        Implement the backward function\n",
    "\n",
    "        Args:\n",
    "            pred_x_start (torch.Tensor): predicted image at timestep 0\n",
    "            t            (int)         : current timestep\n",
    "            t_prev       (int)         : timestep after denoising\n",
    "            pred_noise   (torch.Tensor): predicted noise at timestep t\n",
    "            noise        (torch.Tensor): a random noise\n",
    "            eta          (float)       : scaling factor for noise\n",
    "\n",
    "        Returns:\n",
    "            x_prev     (torch.Tensor): denoised image at timestep t_prev\n",
    "            \n",
    "        Shapes:\n",
    "            pred_x_start : (batch_size, channels, IMG_SIZE, IMG_SIZE)\n",
    "            pred_noise   : (batch_size, channels, IMG_SIZE, IMG_SIZE)\n",
    "            noise        : (batch_size, channels, IMG_SIZE, IMG_SIZE)\n",
    "            \n",
    "            e.g. (32, 3, 64, 64)\n",
    "        \"\"\"\n",
    "        \n",
    "        ##############################\n",
    "        # TODO1-1-3: Denoising Process - Backward function\n",
    "        # Begin your code\n",
    "        \n",
    "        # Handle boundary case for t_prev = -1 (final step)\n",
    "        if t_prev < 0:\n",
    "            return pred_x_start\n",
    "        \n",
    "        # Get alpha_bar values\n",
    "        if t_prev >= 0:\n",
    "            alphas_bar_prev = self.alphas_bar[t_prev]\n",
    "        else:\n",
    "            alphas_bar_prev = torch.tensor(1.0, device=pred_x_start.device)\n",
    "        \n",
    "        alphas_bar_t = self.alphas_bar[t]\n",
    "        \n",
    "        sigma_t = eta * torch.sqrt((1 - alphas_bar_prev) / (1 - alphas_bar_t) * (1 - alphas_bar_t / alphas_bar_prev))\n",
    "        \n",
    "        # DDIM formula\n",
    "        mu_t = expand(torch.sqrt(alphas_bar_prev)) * pred_x_start + expand(torch.sqrt(1 - alphas_bar_prev - sigma_t**2)) * pred_noise\n",
    "        x_prev = (mu_t + expand(sigma_t) * noise)\n",
    "        \n",
    "        return x_prev\n",
    "\n",
    "        # End your code\n",
    "        ##############################\n",
    "\n",
    "    def predict_start_from_noise(self, x_t, t, noise):\n",
    "        c1 = torch.sqrt(1. / self.alphas_bar[t])\n",
    "        c2 = torch.sqrt(1. / self.alphas_bar[t] - 1)\n",
    "        x_start = expand(c1) * x_t - expand(c2) * noise\n",
    "        return x_start\n",
    "\n",
    "    def predict_noise_from_start(self, x_t, t, x0):\n",
    "        c1 = torch.sqrt(1. / self.alphas_bar[t])\n",
    "        c2 = torch.sqrt(1. / self.alphas_bar[t] - 1)\n",
    "        noise = (expand(c1) *x_t - x0) / expand(c2)\n",
    "        return noise\n",
    "\n",
    "    def predict_v(self, x_start, t, noise):\n",
    "        c1 = torch.sqrt(self.alphas_bar[t])\n",
    "        c2 = torch.sqrt(1. - self.alphas_bar[t])\n",
    "        v = expand(c1) * noise - expand(c2) * x_start\n",
    "        return v\n",
    "\n",
    "    def predict_start_from_v(self, x_t, t, v):\n",
    "        c1 = torch.sqrt(self.alphas_bar[t])\n",
    "        c2 = torch.sqrt(1. - self.alphas_bar[t])\n",
    "        x_start = expand(c1) * x_t - expand(c2) * v\n",
    "        return x_start\n",
    "\n",
    "    def q_posterior(self, x_start, x_t, t):\n",
    "        c1 = self.betas * torch.sqrt(self.alphas_bar_prev) / (1. - self.alphas_bar)\n",
    "        c2 = (1. - self.alphas_bar_prev) * torch.sqrt(self.alphas) / (1. - self.alphas_bar)\n",
    "        posterior_mean = expand(c1[t]) * x_start + expand(c2[t]) * x_t\n",
    "        posterior_variance = (self.betas * (1. - self.alphas_bar_prev) / (1. - self.alphas_bar))[t]\n",
    "        posterior_log_variance_clipped = torch.log(posterior_variance.clamp(min =1e-20))[t]\n",
    "        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n",
    "\n",
    "    def model_predictions(self, x, t, x_self_cond = None, clip_x_start = False, rederive_pred_noise = False):\n",
    "        ModelPrediction =  namedtuple('ModelPrediction', ['pred_noise', 'pred_x_start'])\n",
    "        model_output = self.model(x, t, x_self_cond)\n",
    "        maybe_clip = partial(torch.clamp, min = -1., max = 1.) if clip_x_start else identity\n",
    "\n",
    "        if self.objective == 'pred_noise':\n",
    "            pred_noise = model_output\n",
    "            x_start = self.predict_start_from_noise(x, t, pred_noise)\n",
    "            x_start = maybe_clip(x_start)\n",
    "\n",
    "            if clip_x_start and rederive_pred_noise:\n",
    "                pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        elif self.objective == 'pred_x0':\n",
    "            x_start = model_output\n",
    "            x_start = maybe_clip(x_start)\n",
    "            pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        elif self.objective == 'pred_v':\n",
    "            v = model_output\n",
    "            x_start = self.predict_start_from_v(x, t, v)\n",
    "            x_start = maybe_clip(x_start)\n",
    "            pred_noise = self.predict_noise_from_start(x, t, x_start)\n",
    "\n",
    "        return ModelPrediction(pred_noise, x_start)\n",
    "\n",
    "    def p_mean_variance(self, x, t, x_self_cond = None, clip_denoised = True):\n",
    "        preds = self.model_predictions(x, t, x_self_cond)\n",
    "        x_start = preds.pred_x_start\n",
    "\n",
    "        if clip_denoised:\n",
    "            x_start.clamp_(-1., 1.)\n",
    "\n",
    "        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start = x_start, x_t = x, t = t)\n",
    "        return model_mean, posterior_variance, posterior_log_variance, x_start\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def p_sample(self, x, t: int, x_self_cond = None):\n",
    "        b, *_, device = *x.shape, self.device\n",
    "        batched_times = torch.full((b,), t, device = device, dtype = torch.long)\n",
    "        model_mean, _, model_log_variance, x_start = self.p_mean_variance(x = x, t = batched_times, x_self_cond = x_self_cond, clip_denoised = True)\n",
    "        noise = torch.randn_like(x) if t > 0 else 0. # no noise if t == 0\n",
    "        pred_img = model_mean + (0.5 * model_log_variance).exp() * noise\n",
    "        return pred_img, x_start\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def p_sample_loop(self, shape, return_all_timesteps = False):\n",
    "        batch, device = shape[0], self.device\n",
    "\n",
    "        img = torch.randn(shape, device = device)\n",
    "        imgs = [img]\n",
    "        \n",
    "        x_start = None\n",
    "\n",
    "        for t in tqdm(reversed(range(0, self.num_timesteps)), desc = 'sampling loop time step', total = self.num_timesteps):\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            img, x_start = self.p_sample(img, t, self_cond)\n",
    "            imgs.append(img)\n",
    "        \n",
    "        ret = img if not return_all_timesteps else torch.stack(imgs, dim = 1)\n",
    "\n",
    "        ret = self.unnormalize(ret)\n",
    "        return ret\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def ddim_sample(self, shape, return_all_timesteps = False):\n",
    "        batch, device, total_timesteps, sampling_timesteps, eta, objective = shape[0], self.device, self.num_timesteps, self.sampling_timesteps, self.ddim_sampling_eta, self.objective\n",
    "\n",
    "        times = torch.linspace(-1, total_timesteps - 1, steps = sampling_timesteps + 1)   # [-1, 0, 1, 2, ..., T-1] when sampling_timesteps == total_timesteps\n",
    "        times = list(reversed(times.int().tolist()))\n",
    "        time_pairs = list(zip(times[:-1], times[1:])) # [(T-1, T-2), (T-2, T-3), ..., (1, 0), (0, -1)]\n",
    "\n",
    "        img = torch.randn(shape, device = device)\n",
    "        imgs = [img]\n",
    "        \n",
    "        samples = [img[:5]]  # Store initial noisy samples\n",
    "        x_start = None\n",
    "        \n",
    "        ##############################\n",
    "        # TODO1-2-1:  Result Visualization - Denoising Progress\n",
    "        # The following code is a denoising loop from a completed\n",
    "        # noise to an anime face image. You need to visualize the\n",
    "        # denoising progress from this code with 5 different images.\n",
    "\n",
    "        # Begin your code\n",
    "        \n",
    "        step_interval = max(1, len(time_pairs) // 20)\n",
    "    \n",
    "        for step_idx, (time, time_prev) in enumerate(tqdm(time_pairs, desc='sampling loop time step')):\n",
    "            # 修正：確保時間步在有效範圍內\n",
    "            time = max(0, min(time, total_timesteps - 1))\n",
    "            time_cond = torch.full((batch,), time, device=device, dtype=torch.long)\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            pred_noise, x_start, *_ = self.model_predictions(img, time_cond, self_cond, clip_x_start=True, rederive_pred_noise=True)\n",
    "            \n",
    "            noise = torch.randn_like(x_start)\n",
    "            img = self.denoise_backward(x_start, time, time_prev, pred_noise, noise, eta)\n",
    "            imgs.append(img)\n",
    "            \n",
    "            # store samples at specific intervals\n",
    "            if step_idx % step_interval == 0 or step_idx == len(time_pairs) - 1:\n",
    "                samples.append(img[:5])\n",
    "        \n",
    "        ret = img if not return_all_timesteps else torch.stack(imgs, dim=1)\n",
    "        ret = self.unnormalize(ret)\n",
    "        \n",
    "        \n",
    "        num_steps = len(samples) - 1\n",
    "        fig, axes = plt.subplots(5, num_steps + 1, figsize=(2*(num_steps + 1), 10), squeeze=False)\n",
    "        \n",
    "        for col, sample in enumerate(samples):\n",
    "            sample_unnorm = self.unnormalize(sample)  # [5, C, H, W]\n",
    "            sample_unnorm = torch.clamp(sample_unnorm, 0.0, 1.0)\n",
    "        \n",
    "            for row in range(5):\n",
    "                img_single = sample_unnorm[row]  # [C, H, W]\n",
    "                axes[row, col].imshow(img_single.permute(1,2,0).cpu())\n",
    "                axes[row, col].axis('off')\n",
    "                if row == 0:\n",
    "                    axes[row, col].set_title(f\"Step {col}\")\n",
    "        \n",
    "        plt.tight_layout() \n",
    "        plt.savefig(\"denoising_progress.png\")\n",
    "        plt.show()\n",
    "        \n",
    "        # End your code\n",
    "        ##############################\n",
    "        return ret\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def sample(self, batch_size = 16, return_all_timesteps = False):\n",
    "        (h, w), channels = self.image_size, self.channels\n",
    "        sample_fn = self.p_sample_loop if not self.is_ddim_sampling else self.ddim_sample\n",
    "        return sample_fn((batch_size, channels, h, w), return_all_timesteps = return_all_timesteps)\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def interpolate(self, x1, x2, t = None, lam = 0.5):\n",
    "        b, *_, device = *x1.shape, x1.device\n",
    "        t = default(t, self.num_timesteps - 1)\n",
    "\n",
    "        assert x1.shape == x2.shape\n",
    "\n",
    "        t_batched = torch.full((b,), t, device = device)\n",
    "        xt1, xt2 = map(lambda x: self.q_sample(x, t = t_batched), (x1, x2))\n",
    "\n",
    "        img = (1 - lam) * xt1 + lam * xt2\n",
    "\n",
    "        x_start = None\n",
    "\n",
    "        for i in tqdm(reversed(range(0, t)), desc = 'interpolation sample time step', total = t):\n",
    "            self_cond = x_start if self.self_condition else None\n",
    "            img, x_start = self.p_sample(img, i, self_cond)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def noise_assignment(self, x_start, noise):\n",
    "        x_start, noise = tuple(rearrange(t, 'b ... -> b (...)') for t in (x_start, noise))\n",
    "        dist = torch.cdist(x_start, noise)\n",
    "        _, assign = linear_sum_assignment(dist.cpu())\n",
    "        return torch.from_numpy(assign).to(dist.device)\n",
    "\n",
    "    @autocast('cuda', enabled = False)\n",
    "    def q_sample(self, x_start, t, noise = None):\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "        \n",
    "        return self.add_noise_forward(x_start, t, noise)\n",
    "\n",
    "    def p_losses(self, x_start, t, noise = None):\n",
    "        b, c, h, w = x_start.shape\n",
    "\n",
    "        # noise sample\n",
    "\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "        x = self.q_sample(x_start = x_start, t = t, noise = noise)\n",
    "\n",
    "        # if doing self-conditioning, 50% of the time, predict x_start from current set of times\n",
    "        # and condition with unet with that\n",
    "        # this technique will slow down training by 25%, but seems to lower FID significantly\n",
    "\n",
    "        x_self_cond = None\n",
    "        if self.self_condition and random.random() < 0.5:\n",
    "            with torch.no_grad():\n",
    "                x_self_cond = self.model_predictions(x, t).pred_x_start\n",
    "                x_self_cond.detach_()\n",
    "\n",
    "        # predict and take gradient step\n",
    "\n",
    "        model_out = self.model(x, t, x_self_cond)\n",
    "\n",
    "        if self.objective == 'pred_noise':\n",
    "            target = noise\n",
    "        elif self.objective == 'pred_x0':\n",
    "            target = x_start\n",
    "        elif self.objective == 'pred_v':\n",
    "            v = self.predict_v(x_start, t, noise)\n",
    "            target = v\n",
    "        else:\n",
    "            raise ValueError(f'unknown objective {self.objective}')\n",
    "\n",
    "        loss = F.mse_loss(model_out, target, reduction = 'none')\n",
    "        loss = reduce(loss, 'b ... -> b', 'mean')\n",
    "\n",
    "        loss = loss * extract(self.loss_weight, t, loss.shape)\n",
    "        return loss.mean()\n",
    "\n",
    "    def forward(self, img, *args, **kwargs):\n",
    "        b, c, h, w, device, img_size, = *img.shape, img.device, self.image_size\n",
    "        assert h == img_size[0] and w == img_size[1], f'height and width of image must be {img_size}'\n",
    "        t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n",
    "\n",
    "        img = self.normalize(img)\n",
    "        return self.p_losses(img, t, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T07:59:39.123844Z",
     "iopub.status.busy": "2025-06-12T07:59:39.123651Z",
     "iopub.status.idle": "2025-06-12T07:59:39.142657Z",
     "shell.execute_reply": "2025-06-12T07:59:39.141960Z",
     "shell.execute_reply.started": "2025-06-12T07:59:39.123819Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        folder,\n",
    "        image_size\n",
    "    ):\n",
    "        self.folder = folder\n",
    "        self.image_size = image_size\n",
    "        self.paths = [p for p in Path(f'{folder}').glob(f'**/*.jpg')]\n",
    "        \n",
    "        ########## Data Augmentation ##########\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize(self.image_size),\n",
    "            T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "            T.RandomHorizontalFlip(p=0.5),  # 50% 機率進行水平翻轉\n",
    "            T.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index]\n",
    "        img = Image.open(path)\n",
    "        return self.transform(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T07:59:39.144604Z",
     "iopub.status.busy": "2025-06-12T07:59:39.144371Z",
     "iopub.status.idle": "2025-06-12T07:59:39.169412Z",
     "shell.execute_reply": "2025-06-12T07:59:39.168831Z",
     "shell.execute_reply.started": "2025-06-12T07:59:39.144586Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from einops import rearrange, repeat\n",
    "from pytorch_fid.fid_score import calculate_frechet_distance\n",
    "from pytorch_fid.inception import InceptionV3\n",
    "from torch.nn.functional import adaptive_avg_pool2d\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def num_to_groups(num, divisor):\n",
    "    groups = num // divisor\n",
    "    remainder = num % divisor\n",
    "    arr = [divisor] * groups\n",
    "    if remainder > 0:\n",
    "        arr.append(remainder)\n",
    "    return arr\n",
    "\n",
    "\n",
    "class FIDEvaluation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size,\n",
    "        dl,\n",
    "        sampler,\n",
    "        channels=3,\n",
    "        accelerator=None,\n",
    "        stats_dir=\"./results\",\n",
    "        device=\"cuda\",\n",
    "        num_fid_samples=50000,\n",
    "        inception_block_idx=2048,\n",
    "    ):\n",
    "        self.batch_size = batch_size\n",
    "        self.n_samples = num_fid_samples\n",
    "        self.device = device\n",
    "        self.channels = channels\n",
    "        self.dl = dl\n",
    "        self.sampler = sampler\n",
    "        self.stats_dir = stats_dir\n",
    "        self.print_fn = print if accelerator is None else accelerator.print\n",
    "        assert inception_block_idx in InceptionV3.BLOCK_INDEX_BY_DIM\n",
    "        block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[inception_block_idx]\n",
    "        self.inception_v3 = InceptionV3([block_idx]).to(device)\n",
    "        self.dataset_stats_loaded = False\n",
    "\n",
    "    def calculate_inception_features(self, samples):\n",
    "        if self.channels == 1:\n",
    "            samples = repeat(samples, \"b 1 ... -> b c ...\", c=3)\n",
    "\n",
    "        self.inception_v3.eval()\n",
    "        features = self.inception_v3(samples)[0]\n",
    "\n",
    "        if features.size(2) != 1 or features.size(3) != 1:\n",
    "            features = adaptive_avg_pool2d(features, output_size=(1, 1))\n",
    "        features = rearrange(features, \"... 1 1 -> ...\")\n",
    "        return features\n",
    "\n",
    "    def load_or_precalc_dataset_stats(self):\n",
    "        path = os.path.join(self.stats_dir, \"dataset_stats\")\n",
    "        try:\n",
    "            ckpt = np.load(path + \".npz\")\n",
    "            self.m2, self.s2 = ckpt[\"m2\"], ckpt[\"s2\"]\n",
    "            self.print_fn(\"Dataset stats loaded from disk.\")\n",
    "            ckpt.close()\n",
    "        except OSError:\n",
    "            num_batches = int(math.ceil(self.n_samples / self.batch_size))\n",
    "            stacked_real_features = []\n",
    "            self.print_fn(\n",
    "                f\"Stacking Inception features for {self.n_samples} samples from the real dataset.\"\n",
    "            )\n",
    "            for _ in tqdm(range(num_batches)):\n",
    "                try:\n",
    "                    real_samples = next(self.dl)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "                real_samples = real_samples.to(self.device)\n",
    "                real_features = self.calculate_inception_features(real_samples)\n",
    "                stacked_real_features.append(real_features)\n",
    "            stacked_real_features = (\n",
    "                torch.cat(stacked_real_features, dim=0).cpu().numpy()\n",
    "            )\n",
    "            m2 = np.mean(stacked_real_features, axis=0)\n",
    "            s2 = np.cov(stacked_real_features, rowvar=False)\n",
    "            np.savez_compressed(path, m2=m2, s2=s2)\n",
    "            self.print_fn(f\"Dataset stats cached to {path}.npz for future use.\")\n",
    "            self.m2, self.s2 = m2, s2\n",
    "        self.dataset_stats_loaded = True\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def fid_score(self):\n",
    "        if not self.dataset_stats_loaded:\n",
    "            self.load_or_precalc_dataset_stats()\n",
    "        self.sampler.eval()\n",
    "        batches = num_to_groups(self.n_samples, self.batch_size)\n",
    "        stacked_fake_features = []\n",
    "        self.print_fn(\n",
    "            f\"Stacking Inception features for {self.n_samples} generated samples.\"\n",
    "        )\n",
    "        for batch in tqdm(batches):\n",
    "            fake_samples = self.sampler.sample(batch_size=batch)\n",
    "            fake_features = self.calculate_inception_features(fake_samples)\n",
    "            stacked_fake_features.append(fake_features)\n",
    "        stacked_fake_features = torch.cat(stacked_fake_features, dim=0).cpu().numpy()\n",
    "        m1 = np.mean(stacked_fake_features, axis=0)\n",
    "        s1 = np.cov(stacked_fake_features, rowvar=False)\n",
    "\n",
    "        return calculate_frechet_distance(m1, s1, self.m2, self.s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T07:59:39.170561Z",
     "iopub.status.busy": "2025-06-12T07:59:39.170294Z",
     "iopub.status.idle": "2025-06-12T07:59:39.191066Z",
     "shell.execute_reply": "2025-06-12T07:59:39.190460Z",
     "shell.execute_reply.started": "2025-06-12T07:59:39.170542Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        diffusion_model,\n",
    "        folder,\n",
    "        *,\n",
    "        train_batch_size = 16,\n",
    "        gradient_accumulate_every = 1,\n",
    "        train_lr = 1e-3,\n",
    "        train_num_steps = 10000,\n",
    "        ema_update_every = 10,\n",
    "        ema_decay = 0.995,\n",
    "        adam_betas = (0.9, 0.99),\n",
    "        save_and_sample_every = 1000,\n",
    "        num_samples = 25,\n",
    "        results_folder = './results',\n",
    "        amp = False,\n",
    "        mixed_precision_type = 'fp16',\n",
    "        split_batches = True,\n",
    "        convert_image_to = None,\n",
    "        calculate_fid = True,\n",
    "        inception_block_idx = 2048,\n",
    "        max_grad_norm = 1.,\n",
    "        num_fid_samples = 1000,\n",
    "        save_best_and_latest_only = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.losses = []\n",
    "\n",
    "        # accelerator\n",
    "\n",
    "        self.accelerator = Accelerator(\n",
    "            split_batches = split_batches,\n",
    "            mixed_precision = mixed_precision_type if amp else 'no'\n",
    "        )\n",
    "\n",
    "        # model\n",
    "\n",
    "        self.model = diffusion_model\n",
    "        self.channels = diffusion_model.channels\n",
    "        is_ddim_sampling = diffusion_model.is_ddim_sampling\n",
    "\n",
    "        # default convert_image_to depending on channels\n",
    "\n",
    "        if not exists(convert_image_to):\n",
    "            convert_image_to = {1: 'L', 3: 'RGB', 4: 'RGBA'}.get(self.channels)\n",
    "\n",
    "        # sampling and training hyperparameters\n",
    "\n",
    "        assert (math.sqrt(num_samples) ** 2) == num_samples, 'number of samples must have an integer square root'\n",
    "        self.num_samples = num_samples\n",
    "        self.save_and_sample_every = save_and_sample_every\n",
    "\n",
    "        self.batch_size = train_batch_size\n",
    "        self.gradient_accumulate_every = gradient_accumulate_every\n",
    "        assert (train_batch_size * gradient_accumulate_every) >= 16, f'your effective batch size (train_batch_size x gradient_accumulate_every) should be at least 16 or above'\n",
    "\n",
    "        self.train_num_steps = train_num_steps\n",
    "        self.image_size = diffusion_model.image_size\n",
    "\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "\n",
    "        # dataset and dataloader\n",
    "\n",
    "        self.ds = Dataset(folder, self.image_size)\n",
    "\n",
    "        assert len(self.ds) >= 100, 'you should have at least 100 images in your folder. at least 10k images recommended'\n",
    "\n",
    "        dl = DataLoader(self.ds, batch_size = train_batch_size, shuffle = True, pin_memory = True, num_workers = cpu_count())\n",
    "\n",
    "        dl = self.accelerator.prepare(dl)\n",
    "        self.dl = cycle(dl)\n",
    "\n",
    "        # optimizer\n",
    "\n",
    "        self.opt = Adam(diffusion_model.parameters(), lr = train_lr, betas = adam_betas)\n",
    "\n",
    "        # for logging results in a folder periodically\n",
    "\n",
    "        if self.accelerator.is_main_process:\n",
    "            self.ema = EMA(diffusion_model, beta = ema_decay, update_every = ema_update_every)\n",
    "            self.ema.to(self.device)\n",
    "\n",
    "        self.results_folder = Path(results_folder)\n",
    "        self.results_folder.mkdir(exist_ok = True)\n",
    "\n",
    "        # step counter state\n",
    "\n",
    "        self.step = 0\n",
    "\n",
    "        # prepare model, dataloader, optimizer with accelerator\n",
    "\n",
    "        self.model, self.opt = self.accelerator.prepare(self.model, self.opt)\n",
    "\n",
    "        # FID-score computation\n",
    "\n",
    "        self.calculate_fid = calculate_fid and self.accelerator.is_main_process\n",
    "\n",
    "        if self.calculate_fid:\n",
    "            if not is_ddim_sampling:\n",
    "                self.accelerator.print(\n",
    "                    \"WARNING: Robust FID computation requires a lot of generated samples and can therefore be very time consuming.\"\\\n",
    "                    \"Consider using DDIM sampling to save time.\"\n",
    "                )\n",
    "\n",
    "            self.fid_scorer = FIDEvaluation(\n",
    "                batch_size=self.batch_size,\n",
    "                dl=self.dl,\n",
    "                sampler=self.ema.ema_model,\n",
    "                channels=self.channels,\n",
    "                accelerator=self.accelerator,\n",
    "                stats_dir=results_folder,\n",
    "                device=self.device,\n",
    "                num_fid_samples=num_fid_samples,\n",
    "                inception_block_idx=inception_block_idx\n",
    "            )\n",
    "\n",
    "        if save_best_and_latest_only:\n",
    "            assert calculate_fid, \"`calculate_fid` must be True to provide a means for model evaluation for `save_best_and_latest_only`.\"\n",
    "            self.best_fid = 1e10 # infinite\n",
    "\n",
    "        self.save_best_and_latest_only = save_best_and_latest_only\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.accelerator.device\n",
    "\n",
    "    def save(self, milestone):\n",
    "        if not self.accelerator.is_local_main_process:\n",
    "            return\n",
    "\n",
    "        data = {\n",
    "            'step': self.step,\n",
    "            'model': self.accelerator.get_state_dict(self.model),\n",
    "            'opt': self.opt.state_dict(),\n",
    "            'ema': self.ema.state_dict(),\n",
    "            'scaler': self.accelerator.scaler.state_dict() if exists(self.accelerator.scaler) else None\n",
    "        }\n",
    "\n",
    "        torch.save(data, str(self.results_folder / f'model-{milestone}.pt'))\n",
    "\n",
    "    def load(self, milestone):\n",
    "        accelerator = self.accelerator\n",
    "        device = accelerator.device\n",
    "\n",
    "        data = torch.load(str(self.results_folder / f'model-{milestone}.pt'), map_location=device, weights_only=True)\n",
    "\n",
    "        model = self.accelerator.unwrap_model(self.model)\n",
    "        model.load_state_dict(data['model'])\n",
    "\n",
    "        self.step = data['step']\n",
    "        self.opt.load_state_dict(data['opt'])\n",
    "        if self.accelerator.is_main_process:\n",
    "            self.ema.load_state_dict(data[\"ema\"])\n",
    "\n",
    "        if exists(self.accelerator.scaler) and exists(data['scaler']):\n",
    "            self.accelerator.scaler.load_state_dict(data['scaler'])\n",
    "\n",
    "    def train(self):\n",
    "        accelerator = self.accelerator\n",
    "        device = accelerator.device\n",
    "\n",
    "        losses = []\n",
    "        \n",
    "        with tqdm(initial = self.step, total = self.train_num_steps, disable = not accelerator.is_main_process) as pbar:\n",
    "\n",
    "            while self.step < self.train_num_steps:\n",
    "                self.model.train()\n",
    "\n",
    "                total_loss = 0.\n",
    "\n",
    "                for _ in range(self.gradient_accumulate_every):\n",
    "                    data = next(self.dl).to(device)\n",
    "\n",
    "                    with self.accelerator.autocast():\n",
    "                        loss = self.model(data)\n",
    "                        loss = loss / self.gradient_accumulate_every\n",
    "                        total_loss += loss.item()\n",
    "\n",
    "                    self.accelerator.backward(loss)\n",
    "\n",
    "                pbar.set_description(f'loss: {total_loss:.4f}')\n",
    "\n",
    "                accelerator.wait_for_everyone()\n",
    "                accelerator.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
    "\n",
    "                self.opt.step()\n",
    "                self.opt.zero_grad()\n",
    "\n",
    "                accelerator.wait_for_everyone()\n",
    "\n",
    "                self.step += 1\n",
    "                if accelerator.is_main_process:\n",
    "                    losses.append(total_loss)\n",
    "                    self.ema.update()\n",
    "\n",
    "                    if self.step != 0 and divisible_by(self.step, self.save_and_sample_every):\n",
    "                        self.losses.append(sum(losses))\n",
    "                        losses = []\n",
    "                        self.ema.ema_model.eval()\n",
    "\n",
    "                        with torch.inference_mode():\n",
    "                            milestone = self.step // self.save_and_sample_every\n",
    "                            batches = num_to_groups(self.num_samples, self.batch_size)\n",
    "                            all_images_list = list(map(lambda n: self.ema.ema_model.sample(batch_size=n), batches))\n",
    "\n",
    "                        all_images = torch.cat(all_images_list, dim = 0)\n",
    "\n",
    "                        utils.save_image(all_images, str(self.results_folder / f'sample-{milestone}.png'), nrow = int(math.sqrt(self.num_samples)))\n",
    "\n",
    "                        # whether to calculate fid\n",
    "\n",
    "                        if self.calculate_fid:\n",
    "                            fid_score = self.fid_scorer.fid_score()\n",
    "                            accelerator.print(f'fid_score: {fid_score}')\n",
    "\n",
    "                        if self.save_best_and_latest_only:\n",
    "                            if self.best_fid > fid_score:\n",
    "                                self.best_fid = fid_score\n",
    "                                self.save(\"best\")\n",
    "                            self.save(\"latest\")\n",
    "                        else:\n",
    "                            self.save(milestone)\n",
    "\n",
    "                pbar.update(1)\n",
    "\n",
    "        accelerator.print('training complete')\n",
    "    \n",
    "    def inference(self, num=1000, n_iter=5, output_path='./generated'):\n",
    "        if not os.path.exists(output_path):\n",
    "            os.mkdir(output_path)\n",
    "        with torch.no_grad():\n",
    "            for i in range(n_iter):\n",
    "                batches = num_to_groups(num // n_iter, 200)\n",
    "                all_images = list(map(lambda n: self.ema.ema_model.sample(batch_size=n), batches))[0]\n",
    "                for j in range(all_images.size(0)):\n",
    "                    torchvision.utils.save_image(all_images[j], f'{output_path}/{i * 200 + j + 1}.jpg')\n",
    "                    \n",
    "    def plot_losses(self):\n",
    "        ##############################\n",
    "        # TODO1-2-2:  Result Visualization - Loss curve\n",
    "        # You can use self.losses directly to plot the loss curve.\n",
    "        # Begin your code\n",
    "        \n",
    "        losses = self.losses\n",
    "        if isinstance(losses, torch.Tensor):\n",
    "            losses = losses.cpu().numpy()\n",
    "        else:\n",
    "            losses = [float(l) for l in losses]\n",
    "    \n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(losses, label='Loss')\n",
    "        plt.xlabel('Training Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Training Loss Curve')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # End your code\n",
    "        ##############################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T07:59:39.192228Z",
     "iopub.status.busy": "2025-06-12T07:59:39.191947Z",
     "iopub.status.idle": "2025-06-12T08:03:15.846817Z",
     "shell.execute_reply": "2025-06-12T08:03:15.845883Z",
     "shell.execute_reply.started": "2025-06-12T07:59:39.192201Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# TODO1-3: Evaluation Baseline\n",
    "# This area is for tuning the hyper-parameters to improve performance.\n",
    "# If you want to do further enhancement (e.g. data augmentation), \n",
    "# feel free to implement your ideas anywhere. But remember to \n",
    "# specify the changes in your report.\n",
    "\n",
    "# Begin your code\n",
    "\n",
    "########## Hyper-Parameters ##########\n",
    "seed = 114514\n",
    "path = '/kaggle/input/diffusion/faces/faces'\n",
    "IMG_SIZE = 64\n",
    "batch_size = 32\n",
    "train_num_steps = 30000\n",
    "lr = 2e-4  \n",
    "# =================\n",
    "grad_steps = 2\n",
    "ema_decay = 0.995\n",
    "\n",
    "channels = 64 \n",
    "dim_mults = (1, 2, 4, 8) \n",
    "\n",
    "timesteps = 1000  \n",
    "sampling_timesteps = 500 \n",
    "beta_schedule = 'linear'\n",
    "schedule_fn_kwargs = {'s': 0.008},  \n",
    "\n",
    "# End your code\n",
    "##############################\n",
    "\n",
    "set_seed(seed)\n",
    "\n",
    "model = Unet(\n",
    "    dim = channels,\n",
    "    dim_mults = dim_mults,\n",
    "    flash_attn = True\n",
    ")\n",
    "\n",
    "diffusion = GaussianDiffusion(\n",
    "    model,\n",
    "    image_size = IMG_SIZE,\n",
    "    timesteps = timesteps,                     # number of steps\n",
    "    sampling_timesteps = sampling_timesteps,   # number of sampling timesteps (using ddim for faster inference [see citation for ddim paper])\n",
    "    beta_schedule = beta_schedule\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    diffusion,\n",
    "    path,\n",
    "    train_batch_size = batch_size,\n",
    "    train_lr = lr,\n",
    "    train_num_steps = train_num_steps,         # total training steps\n",
    "    gradient_accumulate_every = grad_steps,    # gradient accumulation steps\n",
    "    ema_decay = ema_decay,                     # exponential moving average decay\n",
    "    amp = True,                                # turn on mixed precision\n",
    "    calculate_fid = False                      # whether to calculate fid during training\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "In the inference section, the stored training model weights are used to generate anime face images in the `results` folder. You can modify the `milestone` to select the model weights from specific epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-12T08:03:15.848271Z",
     "iopub.status.busy": "2025-06-12T08:03:15.847995Z",
     "iopub.status.idle": "2025-06-12T08:03:15.954752Z",
     "shell.execute_reply": "2025-06-12T08:03:15.953731Z",
     "shell.execute_reply.started": "2025-06-12T08:03:15.848239Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "milestone = 35\n",
    "trainer.load(milestone)\n",
    "trainer.inference()\n",
    "!zip -r generated.zip generated\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3065496,
     "sourceId": 5266736,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
